{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# NAME ENTITY RECOGNITION (TASK-1)"
      ],
      "metadata": {
        "id": "AlHBYdakmdTv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p CONLL2003\n",
        "!wget -nc -O CONLL2003/train.txt https://github.com/davidsbatista/NER-datasets/raw/dcb6c7439a7de43abc2448bad5b1d81a47f26c0d/CONLL2003/train.txt\n",
        "!wget -nc -O CONLL2003/valid.txt https://github.com/davidsbatista/NER-datasets/raw/dcb6c7439a7de43abc2448bad5b1d81a47f26c0d/CONLL2003/valid.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhEl8FFpmR2E",
        "outputId": "0ab70dbe-04fe-4d7d-9400-8cbf696e0a4f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-10 09:40:44--  https://github.com/davidsbatista/NER-datasets/raw/dcb6c7439a7de43abc2448bad5b1d81a47f26c0d/CONLL2003/train.txt\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/davidsbatista/NER-datasets/dcb6c7439a7de43abc2448bad5b1d81a47f26c0d/CONLL2003/train.txt [following]\n",
            "--2023-11-10 09:40:45--  https://raw.githubusercontent.com/davidsbatista/NER-datasets/dcb6c7439a7de43abc2448bad5b1d81a47f26c0d/CONLL2003/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3283418 (3.1M) [text/plain]\n",
            "Saving to: ‘CONLL2003/train.txt’\n",
            "\n",
            "CONLL2003/train.txt 100%[===================>]   3.13M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-11-10 09:40:46 (199 MB/s) - ‘CONLL2003/train.txt’ saved [3283418/3283418]\n",
            "\n",
            "--2023-11-10 09:40:46--  https://github.com/davidsbatista/NER-datasets/raw/dcb6c7439a7de43abc2448bad5b1d81a47f26c0d/CONLL2003/valid.txt\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/davidsbatista/NER-datasets/dcb6c7439a7de43abc2448bad5b1d81a47f26c0d/CONLL2003/valid.txt [following]\n",
            "--2023-11-10 09:40:46--  https://raw.githubusercontent.com/davidsbatista/NER-datasets/dcb6c7439a7de43abc2448bad5b1d81a47f26c0d/CONLL2003/valid.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 827441 (808K) [text/plain]\n",
            "Saving to: ‘CONLL2003/valid.txt’\n",
            "\n",
            "CONLL2003/valid.txt 100%[===================>] 808.05K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2023-11-10 09:40:46 (102 MB/s) - ‘CONLL2003/valid.txt’ saved [827441/827441]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this is how you read a file of this kind\n",
        "# one item per line, empty lines between sequences\n",
        "\n",
        "from collections import namedtuple\n",
        "\n",
        "#Same as tuple but the fields are named for convenience\n",
        "#this says we have four fields\n",
        "OneWord=namedtuple(\"OneWord\",[\"word\",\"pos_label\",\"chunk_label\",\"entity_label\"])\n",
        "\n",
        "def read_conll2003(f_name):\n",
        "    \"\"\"Yield complete sentences\"\"\"\n",
        "    current_sentence=[] #This will be a list of (word,label), which we accumulate for each sentence\n",
        "    with open(f_name) as f:\n",
        "        for line in f:\n",
        "            line=line.strip() #drop whitespace\n",
        "            if line.startswith(\"-DOCSTART-\"): #let's not worry about these for the time being\n",
        "                continue\n",
        "            if not line: #sentence break\n",
        "                if current_sentence: #if we gathered a sentence, we should yield it, because a new one starts\n",
        "                    yield current_sentence #much like return, but continues past this line once the element has been consumed\n",
        "                    current_sentence=[] #...and start a new one\n",
        "                continue\n",
        "            #if we made it here, we are on a normal line\n",
        "            columns=line.split() #an actual word line\n",
        "            assert len(columns)==4 #we should have four columns, looking at the data\n",
        "            current_sentence.append(OneWord(*columns)) #* expands columns as arguments to OneWord constructor\n",
        "        else: #for ... else -> the else part is executed once, when \"for\" runs out of elements\n",
        "            if current_sentence: #yield also the last one!\n",
        "                yield current_sentence\n",
        "\n",
        "#Now just read the data in\n",
        "sentences_train=list(read_conll2003(\"CONLL2003/train.txt\"))\n",
        "sentences_dev=list(read_conll2003(\"CONLL2003/valid.txt\"))\n",
        "\n",
        "print(\"First three sentences\")\n",
        "for sent in sentences_dev[:3]:\n",
        "    print(sent)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyYFh3egm-Me",
        "outputId": "516b4e8c-480b-4ebc-d619-1a5d472f2f14"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First three sentences\n",
            "[OneWord(word='CRICKET', pos_label='NNP', chunk_label='B-NP', entity_label='O'), OneWord(word='-', pos_label=':', chunk_label='O', entity_label='O'), OneWord(word='LEICESTERSHIRE', pos_label='NNP', chunk_label='B-NP', entity_label='B-ORG'), OneWord(word='TAKE', pos_label='NNP', chunk_label='I-NP', entity_label='O'), OneWord(word='OVER', pos_label='IN', chunk_label='B-PP', entity_label='O'), OneWord(word='AT', pos_label='NNP', chunk_label='B-NP', entity_label='O'), OneWord(word='TOP', pos_label='NNP', chunk_label='I-NP', entity_label='O'), OneWord(word='AFTER', pos_label='NNP', chunk_label='I-NP', entity_label='O'), OneWord(word='INNINGS', pos_label='NNP', chunk_label='I-NP', entity_label='O'), OneWord(word='VICTORY', pos_label='NN', chunk_label='I-NP', entity_label='O'), OneWord(word='.', pos_label='.', chunk_label='O', entity_label='O')]\n",
            "\n",
            "[OneWord(word='LONDON', pos_label='NNP', chunk_label='B-NP', entity_label='B-LOC'), OneWord(word='1996-08-30', pos_label='CD', chunk_label='I-NP', entity_label='O')]\n",
            "\n",
            "[OneWord(word='West', pos_label='NNP', chunk_label='B-NP', entity_label='B-MISC'), OneWord(word='Indian', pos_label='NNP', chunk_label='I-NP', entity_label='I-MISC'), OneWord(word='all-rounder', pos_label='NN', chunk_label='I-NP', entity_label='O'), OneWord(word='Phil', pos_label='NNP', chunk_label='I-NP', entity_label='B-PER'), OneWord(word='Simmons', pos_label='NNP', chunk_label='I-NP', entity_label='I-PER'), OneWord(word='took', pos_label='VBD', chunk_label='B-VP', entity_label='O'), OneWord(word='four', pos_label='CD', chunk_label='B-NP', entity_label='O'), OneWord(word='for', pos_label='IN', chunk_label='B-PP', entity_label='O'), OneWord(word='38', pos_label='CD', chunk_label='B-NP', entity_label='O'), OneWord(word='on', pos_label='IN', chunk_label='B-PP', entity_label='O'), OneWord(word='Friday', pos_label='NNP', chunk_label='B-NP', entity_label='O'), OneWord(word='as', pos_label='IN', chunk_label='B-PP', entity_label='O'), OneWord(word='Leicestershire', pos_label='NNP', chunk_label='B-NP', entity_label='B-ORG'), OneWord(word='beat', pos_label='VBD', chunk_label='B-VP', entity_label='O'), OneWord(word='Somerset', pos_label='NNP', chunk_label='B-NP', entity_label='B-ORG'), OneWord(word='by', pos_label='IN', chunk_label='B-PP', entity_label='O'), OneWord(word='an', pos_label='DT', chunk_label='B-NP', entity_label='O'), OneWord(word='innings', pos_label='NN', chunk_label='I-NP', entity_label='O'), OneWord(word='and', pos_label='CC', chunk_label='O', entity_label='O'), OneWord(word='39', pos_label='CD', chunk_label='B-NP', entity_label='O'), OneWord(word='runs', pos_label='NNS', chunk_label='I-NP', entity_label='O'), OneWord(word='in', pos_label='IN', chunk_label='B-PP', entity_label='O'), OneWord(word='two', pos_label='CD', chunk_label='B-NP', entity_label='O'), OneWord(word='days', pos_label='NNS', chunk_label='I-NP', entity_label='O'), OneWord(word='to', pos_label='TO', chunk_label='B-VP', entity_label='O'), OneWord(word='take', pos_label='VB', chunk_label='I-VP', entity_label='O'), OneWord(word='over', pos_label='IN', chunk_label='B-PP', entity_label='O'), OneWord(word='at', pos_label='IN', chunk_label='B-PP', entity_label='O'), OneWord(word='the', pos_label='DT', chunk_label='B-NP', entity_label='O'), OneWord(word='head', pos_label='NN', chunk_label='I-NP', entity_label='O'), OneWord(word='of', pos_label='IN', chunk_label='B-PP', entity_label='O'), OneWord(word='the', pos_label='DT', chunk_label='B-NP', entity_label='O'), OneWord(word='county', pos_label='NN', chunk_label='I-NP', entity_label='O'), OneWord(word='championship', pos_label='NN', chunk_label='I-NP', entity_label='O'), OneWord(word='.', pos_label='.', chunk_label='O', entity_label='O')]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sentence_features(sent):\n",
        "    #Given a sentence as a list of (word, label) pairs\n",
        "    #generate the features for every word\n",
        "    #The result should be a list of same length as the sentence\n",
        "    #Each item is a dictionary of {\"feature name\"->feature value} mappings, holding all features of the word at that position\n",
        "\n",
        "    sent_features=[] #this will be the result\n",
        "    for one_word in sent:\n",
        "        #We must do nothing with label\n",
        "        #it just happens to be around\n",
        "        word_features={}\n",
        "        word_features[\"word_\"+one_word.word]=1 #the word itself is a feature\n",
        "        sent_features.append(word_features)\n",
        "    return sent_features\n",
        "\n",
        "print(generate_sentence_features(sentences_dev[0])  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8NpMxFYnK_L",
        "outputId": "c84f1562-d188-465d-8d29-f6182c4c025b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'word_CRICKET': 1}, {'word_-': 1}, {'word_LEICESTERSHIRE': 1}, {'word_TAKE': 1}, {'word_OVER': 1}, {'word_AT': 1}, {'word_TOP': 1}, {'word_AFTER': 1}, {'word_INNINGS': 1}, {'word_VICTORY': 1}, {'word_.': 1}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#...now we can generate the training examples\n",
        "def prep_data(sentences):\n",
        "    all_labels=[] #here we gather labels for all words in all sentences\n",
        "    all_features=[] #here we gather features for all words in all sentences\n",
        "    for sentence in sentences:\n",
        "        sent_features=generate_sentence_features(sentence)\n",
        "        assert len(sent_features)==len(sentence)\n",
        "        #Now we can get, for every position its label and its features\n",
        "        for one_word,features in zip(sentence,sent_features):\n",
        "            all_labels.append(one_word.pos_label) #label\n",
        "            all_features.append(features)         #and features to go with it\n",
        "    return all_labels, all_features\n",
        "\n",
        "train_labels,train_features=prep_data(sentences_train)\n",
        "dev_labels,dev_features=prep_data(sentences_dev)"
      ],
      "metadata": {
        "id": "yOpx6jtbnTjH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction import DictVectorizer\n",
        "vectorizer=DictVectorizer()\n",
        "vectorizer.fit(train_features)\n",
        "print(\"Vectorizer vocab size:\",len(vectorizer.vocabulary_))\n",
        "\n",
        "feature_vectors_train=vectorizer.transform(train_features)\n",
        "feature_vectors_dev=vectorizer.transform(dev_features)\n",
        "\n",
        "print(\"Train shape\",feature_vectors_train.shape)\n",
        "print(\"Dev shape\",feature_vectors_dev.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upmb58hZngXN",
        "outputId": "5f2513de-d348-466b-a0a4-29b1bdcf6641"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorizer vocab size: 23623\n",
            "Train shape (203621, 23623)\n",
            "Dev shape (51362, 23623)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.svm\n",
        "\n",
        "classifier=sklearn.svm.LinearSVC(C=0.05,verbose=1)\n",
        "classifier.fit(feature_vectors_train, train_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "BFi9aM3GnpAC",
        "outputId": "3c17d3e4-dfaf-4d90-ccea-2ea35c0b2c3d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LibLinear]"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVC(C=0.05, verbose=1)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC(C=0.05, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(C=0.05, verbose=1)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.score(feature_vectors_dev,dev_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AW97Gegnxio",
        "outputId": "fcade140-0c57-4507-f66c-477d3fce16f0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8655426190568903"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sentence_features(sent):\n",
        "    #Given a sentence as a list of (word, label) pairs\n",
        "    #generate the features for every word\n",
        "    #The result should be a list of same length as the sentence\n",
        "    #Each item is a dictionary of {\"feature name\"->feature value} mappings, holding all features of the word at that position\n",
        "\n",
        "    sent_features=[] #this will be the result\n",
        "    for word_idx, one_word in enumerate(sent):\n",
        "        #We do nothing with label\n",
        "        #it just happens to be around\n",
        "        word_features={}\n",
        "        word_features[\"word_\"+one_word.word]=1 #the word itself is a feature\n",
        "        if word_idx!=0:\n",
        "            word_features[\"left_word_\"+sent[word_idx-1].word]=1\n",
        "        if word_idx!=len(sent)-1:\n",
        "            word_features[\"right_word_\"+sent[word_idx+1].word]=1\n",
        "        sent_features.append(word_features)\n",
        "    return sent_features\n",
        "\n",
        "train_labels,train_features=prep_data(sentences_train)\n",
        "dev_labels,dev_features=prep_data(sentences_dev)\n",
        "vectorizer=DictVectorizer()\n",
        "vectorizer.fit(train_features)\n",
        "feature_vectors_train=vectorizer.transform(train_features)\n",
        "feature_vectors_dev=vectorizer.transform(dev_features)\n",
        "\n",
        "print(\"Train shape\",feature_vectors_train.shape)\n",
        "print(\"Dev shape\",feature_vectors_dev.shape)\n",
        "\n",
        "classifier=sklearn.svm.LinearSVC(C=1,verbose=1)\n",
        "classifier.fit(feature_vectors_train, train_labels)\n",
        "classifier.score(feature_vectors_dev,dev_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnOpWoIHn3yd",
        "outputId": "1730cf4b-d850-41d9-ff47-5395dd07ae54"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape (203621, 68467)\n",
            "Dev shape (51362, 68467)\n",
            "[LibLinear]"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9292862427475566"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us try to look at some predictions\n",
        "sentence=\"I can house arrest you in my house .\".split()\n",
        "\n",
        "sentence_data=[OneWord(w,\"XXX\",\"XXX\",\"XXX\") for w in sentence] #we need to fake this a bit, to get data in the correct format\n",
        "_,sentence_features=prep_data([sentence_data])\n",
        "sentence_vectors=vectorizer.transform(sentence_features)\n",
        "predictions=classifier.predict(sentence_vectors)\n",
        "for word,label in zip(sentence,predictions):\n",
        "    print(word,label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pQlEgiroG-c",
        "outputId": "07aee63c-a55d-4788-acea-e09c9124015e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I PRP\n",
            "can MD\n",
            "house VB\n",
            "arrest NN\n",
            "you PRP\n",
            "in IN\n",
            "my PRP$\n",
            "house NN\n",
            ". .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Learned coefficients:\",classifier.coef_.shape)\n",
        "print(\"Classes in the data:\",classifier.classes_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovS9dmnhoX7y",
        "outputId": "0ae8854c-9df7-4171-d452-da77fd2b90f8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned coefficients: (45, 68467)\n",
            "Classes in the data: ['\"' '$' \"''\" '(' ')' ',' '.' ':' 'CC' 'CD' 'DT' 'EX' 'FW' 'IN' 'JJ' 'JJR'\n",
            " 'JJS' 'LS' 'MD' 'NN' 'NNP' 'NNPS' 'NNS' 'NN|SYM' 'PDT' 'POS' 'PRP' 'PRP$'\n",
            " 'RB' 'RBR' 'RBS' 'RP' 'SYM' 'TO' 'UH' 'VB' 'VBD' 'VBG' 'VBN' 'VBP' 'VBZ'\n",
            " 'WDT' 'WP' 'WP$' 'WRB']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "\n",
        "#Reverse the dictionary\n",
        "index2feature={}\n",
        "for feature,idx in vectorizer.vocabulary_.items():\n",
        "    assert idx not in index2feature #This really should hold\n",
        "    index2feature[idx]=feature\n",
        "#Now we can query index2feature to get the feature names as we need\n",
        "\n",
        "i=list(classifier.classes_).index(\"NN\") #which of the coefficients corresponds to nouns?\n",
        "indices=numpy.argsort(classifier.coef_[i])\n",
        "print(\"Negative features\")\n",
        "for idx in indices[:30]:\n",
        "    print(index2feature[idx])\n",
        "print(\"-------------------------------\")\n",
        "print(\"Positive features\")\n",
        "for idx in indices[::-1][:30]: #you can also do it the other way round, reverse, then pick\n",
        "    print(index2feature[idx])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRJFyVpkoaR5",
        "outputId": "fcafced7-887b-4135-88a4-5d0c22530100"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative features\n",
            "left_word_will\n",
            "left_word_Sale\n",
            "word_,\n",
            "left_word_going\n",
            "left_word_could\n",
            "left_word_would\n",
            "left_word_goals\n",
            "right_word_A-rated\n",
            "left_word_We\n",
            "word_and\n",
            "left_word_At\n",
            "word_in\n",
            "left_word_still\n",
            "right_word_announcement\n",
            "left_word_mixer\n",
            "left_word_can\n",
            "left_word_I\n",
            "left_word_should\n",
            "left_word_kms\n",
            "left_word_might\n",
            "left_word_8:00\n",
            "left_word_prices\n",
            "left_word_Mike\n",
            "right_word_SCOREBOARD\n",
            "left_word_must\n",
            "left_word_n't\n",
            "left_word_overs\n",
            "right_word_effect\n",
            "left_word_Services\n",
            "word_two\n",
            "-------------------------------\n",
            "Positive features\n",
            "word_world\n",
            "word_power\n",
            "word_consumer\n",
            "word_peace\n",
            "word_number\n",
            "word_hospital\n",
            "word_vouch\n",
            "word_cricket\n",
            "word_procure\n",
            "word_soccer\n",
            "word_victory\n",
            "word_championship\n",
            "word_staff\n",
            "word_motor\n",
            "word_value\n",
            "word_cabinet\n",
            "word_lunch\n",
            "word_rain\n",
            "word_injury\n",
            "word_league\n",
            "word_anyone\n",
            "word_UNION\n",
            "word_weekend\n",
            "word_edge\n",
            "word_parliament\n",
            "word_shutdown\n",
            "word_division\n",
            "word_cash\n",
            "word_tournament\n",
            "word_race\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NER using spacy\n",
        "\n",
        "!pip install -U spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpRE-sgdojVd",
        "outputId": "cd8a96bd-adfa-44d3-9a2f-5d3c4578a2f9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
            "  Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
            "  Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "Installing collected packages: cloudpathlib, weasel, spacy\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.6.1\n",
            "    Uninstalling spacy-3.6.1:\n",
            "      Successfully uninstalled spacy-3.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.6.0 requires spacy<3.7.0,>=3.6.0, but you have spacy 3.7.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cloudpathlib-0.16.0 spacy-3.7.2 weasel-0.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from __future__ import unicode_literals, print_function\n",
        "\n",
        "import random\n",
        "from pathlib import Path\n",
        "import spacy\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "6Td_nMePo1MZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "NER = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnBhOi0Yo-jZ",
        "outputId": "f41dd7c6-378d-49d8-a5fe-53ff344bf600"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.6.0) was trained with spaCy v3.6.0 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text=\"The Indian Space Research Organisation or is the national space agency of India, headquartered in Bengaluru. It operates under Department of Space which is directly overseen by the Prime Minister of India while Chairman of ISRO acts as executive of DOS as well.\""
      ],
      "metadata": {
        "id": "N8ZQv22BpFBa"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1= NER(raw_text)\n"
      ],
      "metadata": {
        "id": "V33Rln_TpQP3"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in text1.ents:\n",
        "    print(word.text,word.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VtQOT-GpJ4t",
        "outputId": "07ab2857-13cb-43cb-e5a0-e716c36adebd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Indian Space Research Organisation ORG\n",
            "India GPE\n",
            "Bengaluru GPE\n",
            "Department of Space ORG\n",
            "India GPE\n",
            "ISRO ORG\n",
            "DOS ORG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.explain(\"ORG\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "SUov0wLUpWK4",
        "outputId": "d4d82e1a-1fa8-448f-fae6-97fda258f56d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Companies, agencies, institutions, etc.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.explain(\"GPE\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "lDDcmwHjpW4A",
        "outputId": "9690a382-651d-4231-ecc9-feab6a182b9d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Countries, cities, states'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.render(text1,style=\"ent\",jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "djKMdPeupXaV",
        "outputId": "ac4c4449-7e29-42b7-d9b8-bc00c28dc8eb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    The Indian Space Research Organisation\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " or is the national space agency of \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    India\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ", headquartered in \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bengaluru\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ". It operates under \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Department of Space\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " which is directly overseen by the Prime Minister of \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    India\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " while Chairman of \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    ISRO\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " acts as executive of \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    DOS\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " as well.</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text2=\"India is famous for the Taj Mahal.India really is multilingual.India invented yoga.India is the birthplace of Ayurveda.\""
      ],
      "metadata": {
        "id": "spQIXGltpl9j"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text2= NER(raw_text2)"
      ],
      "metadata": {
        "id": "EvAwOxWgpmjr"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in text2.ents:\n",
        "    print(word.text,word.label_)"
      ],
      "metadata": {
        "id": "zVrgyIO2pm5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.render(text2,style=\"ent\",jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "CGwKmYm0p1YH",
        "outputId": "96a300f5-00ed-4840-bfd3-bbbc42a33335"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    India\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " is famous for \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the Taj Mahal\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ".\n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    India\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " really is multilingual.\n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    India\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " invented yoga.\n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    India\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " is the birthplace of \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Ayurveda\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ".</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B5gdrh7Z_Ic",
        "outputId": "90e4e2b1-4c29-4b2f-9f69-205b88eab456",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# PARTS OF SPEECH TAGGING (TASK--2)\n",
        "\n",
        "\n",
        "\n",
        "#This cell loads the Penn Treebank corpus from nltk into a list variable named penn_treebank.\n",
        "\n",
        "#No need to install nltk in google colab since it is preloaded in the environments.\n",
        "#!pip install nltk\n",
        "import nltk\n",
        "\n",
        "#Ensure that the treebank corpus is downloaded\n",
        "nltk.download('treebank')\n",
        "\n",
        "#Load the treebank corpus class\n",
        "from nltk.corpus import treebank\n",
        "\n",
        "#Now we iterate over all samples from the corpus (the fileids - that are equivalent to sentences)\n",
        "#and retrieve the word and the pre-labeled PoS tag. This will be added as a list of tuples with\n",
        "#a list of words and a list of their respective PoS tags (in the same order).\n",
        "penn_treebank = []\n",
        "for fileid in treebank.fileids():\n",
        "  tokens = []\n",
        "  tags = []\n",
        "  for word, tag in treebank.tagged_words(fileid):\n",
        "    tokens.append(word)\n",
        "    tags.append(tag)\n",
        "  penn_treebank.append((tokens, tags))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WwZYkNr1bPN",
        "outputId": "85cdd3aa-980b-44b8-c1d4-f4b5939c2ea9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#This cell loads the Universal Dependecies Treekbank corpus. It'll download all the packages, but we'll only use the GUM\n",
        "#english package. We'll also install the conllu package, that was developed to parse data in the conLLu format, a\n",
        "#format common of linguistic annotated files. We'll also have a list variable, but now named ud_treebank.\n",
        "\n",
        "#Install conllu package, download the UD Treebanks corpus and unpack it.\n",
        "!pip install conllu\n",
        "!wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-3105/ud-treebanks-v2.5.tgz\n",
        "!tar zxf ud-treebanks-v2.5.tgz\n",
        "\n",
        "#The imports needed to open and parse (interpret) the conllu file. At the end we'll have a list of dicts.\n",
        "from io import open\n",
        "from conllu import parse_incr\n",
        "\n",
        "#Open the file and load the sentences to a list.\n",
        "data_file = open(\"ud-treebanks-v2.5/UD_Telugu-MTG/te_mtg-ud-train.conllu\", \"r\", encoding=\"utf-8\")\n",
        "ud_files = []\n",
        "for tokenlist in parse_incr(data_file):\n",
        "    ud_files.append(tokenlist)\n",
        "\n",
        "#Now we iterate over all samples from the corpus and retrieve the word and the pre-labeled PoS tag (upostag). This will\n",
        "#be added as a list of tuples with a list of words and a list of their respective PoS tags (in the same order).\n",
        "ud_treebank = []\n",
        "for sentence in ud_files:\n",
        "  tokens = []\n",
        "  tags = []\n",
        "  for token in sentence:\n",
        "    tokens.append(token['form'])\n",
        "    tags.append(token['upostag'])\n",
        "  ud_treebank.append((tokens, tags))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: conllu in /usr/local/lib/python3.10/dist-packages (4.5.3)\n",
            "--2023-11-10 08:46:41--  https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-3105/ud-treebanks-v2.5.tgz\n",
            "Resolving lindat.mff.cuni.cz (lindat.mff.cuni.cz)... 195.113.20.140\n",
            "Connecting to lindat.mff.cuni.cz (lindat.mff.cuni.cz)|195.113.20.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 355216681 (339M) [application/x-gzip]\n",
            "Saving to: ‘ud-treebanks-v2.5.tgz.1’\n",
            "\n",
            "ud-treebanks-v2.5.t 100%[===================>] 338.76M  12.5MB/s    in 27s     \n",
            "\n",
            "2023-11-10 08:47:09 (12.5 MB/s) - ‘ud-treebanks-v2.5.tgz.1’ saved [355216681/355216681]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IyIaTSwoo-V"
      },
      "source": [
        "#Regex module for checking alphanumeric values.\n",
        "import re\n",
        "def extract_features(sentence, index):\n",
        "  return {\n",
        "      'word':sentence[index],\n",
        "      'is_first':index==0,\n",
        "      'is_last':index ==len(sentence)-1,\n",
        "      'is_capitalized':sentence[index][0].upper() == sentence[index][0],\n",
        "      'is_all_caps': sentence[index].upper() == sentence[index],\n",
        "      'is_all_lower': sentence[index].lower() == sentence[index],\n",
        "      'is_alphanumeric': int(bool((re.match('^(?=.*[0-9]$)(?=.*[a-zA-Z])',sentence[index])))),\n",
        "      'prefix-1':sentence[index][0],\n",
        "      'prefix-2':sentence[index][:2],\n",
        "      'prefix-3':sentence[index][:3],\n",
        "      'prefix-3':sentence[index][:4],\n",
        "      'suffix-1':sentence[index][-1],\n",
        "      'suffix-2':sentence[index][-2:],\n",
        "      'suffix-3':sentence[index][-3:],\n",
        "      'suffix-3':sentence[index][-4:],\n",
        "      'prev_word':'' if index == 0 else sentence[index-1],\n",
        "      'next_word':'' if index < len(sentence) else sentence[index+1],\n",
        "      'has_hyphen': '-' in sentence[index],\n",
        "      'is_numeric': sentence[index].isdigit(),\n",
        "      'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hiniE_wzPOC"
      },
      "source": [
        "#Ater defining the extract_features, we define a simple function to transform our data in a more 'datasetish' format.\n",
        "#This function returns the data as two lists, one of Dicts of features and the other with the labels.\n",
        "def transform_to_dataset(tagged_sentences):\n",
        "  X, y = [], []\n",
        "  for sentence, tags in tagged_sentences:\n",
        "    sent_word_features, sent_tags = [],[]\n",
        "    for index in range(len(sentence)):\n",
        "        sent_word_features.append(extract_features(sentence, index)),\n",
        "        sent_tags.append(tags[index])\n",
        "    X.append(sent_word_features)\n",
        "    y.append(sent_tags)\n",
        "  return X, y\n",
        "\n",
        "#We divide the set BEFORE encoding. Why? To have full sentences in training/testing sets. When we encode, we do not encode\n",
        "#a sentence, but its words instead.\n",
        "\n",
        "#First, for the Penn treebank.\n",
        "penn_train_size = int(0.8*len(penn_treebank))\n",
        "penn_training = penn_treebank[:penn_train_size]\n",
        "penn_testing = penn_treebank[penn_train_size:]\n",
        "X_penn_train, y_penn_train = transform_to_dataset(penn_training)\n",
        "X_penn_test, y_penn_test = transform_to_dataset(penn_testing)\n",
        "\n",
        "#Then, for UD Treebank.\n",
        "ud_train_size = int(0.8*len(ud_treebank))\n",
        "ud_training = ud_treebank[:ud_train_size]\n",
        "ud_testing = ud_treebank[ud_train_size:]\n",
        "X_ud_train, y_ud_train = transform_to_dataset(ud_training)\n",
        "X_ud_test, y_ud_test = transform_to_dataset(ud_testing)\n",
        "\n",
        "#Third step, vectorize datasets. For that we use sklearn DictVectorizer\n",
        "#WARNING"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHTkotyWpd28",
        "outputId": "828f9439-8075-4d19-c68c-3a16db21693d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Ignoring some warnings for the sake of readability.\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#First, install sklearn_crfsuite, as it is not preloaded into Colab.\n",
        "!pip install sklearn_crfsuite\n",
        "from sklearn_crfsuite import CRF\n",
        "\n",
        "#This loads the model. Specifics are:\n",
        "#algorithm: methodology used to check if results are improving. Default is lbfgs (gradient descent).\n",
        "#c1 and c2:  coefficients used for regularization.\n",
        "#max_iterations: max number of iterations (DUH!)\n",
        "#all_possible_transitions: since crf creates a \"network\", of probability transition states,\n",
        "#this option allows it to map even \"connections\" not present in the data.\n",
        "penn_crf = CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.01,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "#The fit method is the default name used by Machine Learning algorithms to start training.\n",
        "print(\"Started training on Penn Treebank corpus!\")\n",
        "penn_crf.fit(X_penn_train, y_penn_train)\n",
        "print(\"Finished training on Penn Treebank corpus!\")\n",
        "\n",
        "#Same for UD\n",
        "ud_crf = CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.01,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "print(\"Started training on UD corpus!\")\n",
        "ud_crf.fit(X_ud_train, y_ud_train)\n",
        "print(\"Finished training on UD corpus!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sklearn_crfsuite\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Collecting python-crfsuite>=0.8.3 (from sklearn_crfsuite)\n",
            "  Downloading python_crfsuite-0.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (1.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (4.66.1)\n",
            "Installing collected packages: python-crfsuite, sklearn_crfsuite\n",
            "Successfully installed python-crfsuite-0.9.9 sklearn_crfsuite-0.3.6\n",
            "Started training on Penn Treebank corpus!\n",
            "Finished training on Penn Treebank corpus!\n",
            "Started training on UD corpus!\n",
            "Finished training on UD corpus!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTlJwNkF_0zs",
        "outputId": "c64ca911-0a95-46eb-8c7a-51e70d049cbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#We'll use the sklearn_crfsuit own metrics to compute f1 score.\n",
        "from sklearn_crfsuite import metrics\n",
        "from sklearn_crfsuite import scorers\n",
        "\n",
        "print(\"## Penn ##\")\n",
        "\n",
        "#First calculate a prediction from test data, then we print the metrics for f-1 using the .flat_f1_score method.\n",
        "y_penn_pred=penn_crf.predict(X_penn_test)\n",
        "print(\"F1 score on Test Data\")\n",
        "print(metrics.flat_f1_score(y_penn_test, y_penn_pred,average='weighted',labels=penn_crf.classes_))\n",
        "#For the sake of clarification, we do the same for train data.\n",
        "y_penn_pred_train=penn_crf.predict(X_penn_train)\n",
        "print(\"F1 score on Training Data \")\n",
        "print(metrics.flat_f1_score(y_penn_train, y_penn_pred_train,average='weighted',labels=penn_crf.classes_))\n",
        "\n",
        "\n",
        "\n",
        "# This presents class wise score. Helps see which classes (tags) are the ones with most problems.\n",
        "print(\"Class wise score:\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Penn ##\n",
            "F1 score on Test Data\n",
            "0.9668646324625245\n",
            "F1 score on Training Data \n",
            "0.9936643188628935\n",
            "Class wise score:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.flat_classification_report(y_penn_test, y_penn_pred, labels=penn_crf.classes_,digits=3))\n",
        "\n",
        "\n",
        "\n",
        "#Same for UD\n",
        "print(\"## UD ##\")\n",
        "\n",
        "y_ud_pred=ud_crf.predict(X_ud_test)\n",
        "print(\"F1 score on Test Data \")\n",
        "print(metrics.flat_f1_score(y_ud_test, y_ud_pred,average='weighted',labels=ud_crf.classes_))\n",
        "y_ud_pred_train=ud_crf.predict(X_ud_train)\n",
        "print(\"F1 score on Training Data \")\n",
        "print(metrics.flat_f1_score(y_ud_train, y_ud_pred_train,average='weighted',labels=ud_crf.classes_))\n",
        "\n",
        "### Look at class wise score\n",
        "print(\"Class wise score:\")\n",
        "print(metrics.flat_classification_report(\n",
        "    y_ud_test, y_ud_pred, labels=ud_crf.classes_, digits=3\n",
        "))\n"
      ],
      "metadata": {
        "id": "Qcy3wQwOhEgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4rcQq-ubbuT",
        "outputId": "41547201-fd00-4304-e5ff-1df1938cb04b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#First, we pass the sentence and \"quickly tokenize it\" - we've already done it in our code, so I'll just mock here with a split:\n",
        "sent = \"తెలుగు అనేది ద్రావిడ భాషల కుటుంబానికి చెందిన భాష. దీనిని మాట్లాడే ప్రజలు ప్రధానంగా ఆంధ్ర, తెలంగాణాలో ఉన్నారు. ఇది ఆ రాష్ట్రాలలో అధికార భాష\"\n",
        "features = [extract_features(sent.split(), idx) for idx in range(len(sent.split()))]\n",
        "\n",
        "#Then we tell the algorithm to make a prediction on a single input (sentence). I'll do once for Penn Treebank and once for UD.\n",
        "penn_results = penn_crf.predict_single(features)\n",
        "ud_results = ud_crf.predict_single(features)\n",
        "\n",
        "#These line magics are just there to make it a neaty print, making a (word, POS) style print;\n",
        "penn_tups = [(sent.split()[idx], penn_results[idx]) for idx in range(len(sent.split()))]\n",
        "ud_tups = [(sent.split()[idx], ud_results[idx]) for idx in range(len(sent.split()))]\n",
        "\n",
        "#The results come out here! Notice the difference in tags.\n",
        "print(penn_tups)\n",
        "print(ud_tups)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('తెలుగు', 'CD'), ('అనేది', 'CD'), ('ద్రావిడ', 'CD'), ('భాషల', 'CD'), ('కుటుంబానికి', 'CD'), ('చెందిన', 'CD'), ('భాష.', 'CD'), ('దీనిని', 'CD'), ('మాట్లాడే', 'CD'), ('ప్రజలు', 'CD'), ('ప్రధానంగా', 'CD'), ('ఆంధ్ర,', 'CD'), ('తెలంగాణాలో', 'CD'), ('ఉన్నారు.', 'CD'), ('ఇది', 'CD'), ('ఆ', 'CD'), ('రాష్ట్రాలలో', 'CD'), ('అధికార', 'CD'), ('భాష', 'CD')]\n",
            "[('తెలుగు', 'PROPN'), ('అనేది', 'VERB'), ('ద్రావిడ', 'ADP'), ('భాషల', 'NOUN'), ('కుటుంబానికి', 'NOUN'), ('చెందిన', 'VERB'), ('భాష.', 'NOUN'), ('దీనిని', 'NOUN'), ('మాట్లాడే', 'VERB'), ('ప్రజలు', 'NOUN'), ('ప్రధానంగా', 'ADV'), ('ఆంధ్ర,', 'NOUN'), ('తెలంగాణాలో', 'NOUN'), ('ఉన్నారు.', 'VERB'), ('ఇది', 'PRON'), ('ఆ', 'DET'), ('రాష్ట్రాలలో', 'NOUN'), ('అధికార', 'VERB'), ('భాష', 'PUNCT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ul2KlQu4c5-N"
      },
      "source": [
        "#import the pickle module\n",
        "import pickle\n",
        "\n",
        "#Simply dump! Use 'wb' in open to write bytes.\n",
        "\n",
        "penn_filename = 'penn_treebank_crf_postagger.sav'\n",
        "pickle.dump(penn_crf, open(penn_filename, 'wb'))\n",
        "\n",
        "ud_filename = 'ud_crf_postagger.sav'\n",
        "pickle.dump(ud_crf, open(ud_filename,'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}